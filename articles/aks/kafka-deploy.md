In this article, you deploy a highly available Kafka cluster on AKS.

* If you haven't already created the required infrastructure for this deployment, follow the steps in [Create infrastructure for deploying a highly available Kafka cluster on AKS][kafka-infrastructure] to get set up, and then you can return to this article.

## Strimzi Cluster Operator Deployment

The Strimzi Cluster Operator is deployed to its own namespace, *Strimzi Operator*, and is configured to watch the namespace, *Kafka*, where the Kafka cluster components will be deployed to. To ensure high availability, the operator is deployed with multiple replicas and with leader election enabled to allow for multiple replicas to run concurrently. One replica is chosen as the active leader to manage the deployed resources, while the others remain on standby. If the leader stops functioning or fails, a standby replica is elected to take over and manage the resources. To ensure availability in case of a zonal outage, three replicas are required, one per availability zone. A pod anti-affinity rule is configured to prevent replicas from being created in the same availability zone. A pod disruption budget is also generated by the Cluster Operator deployment and ensures that at minimum one replica is available at all times. 

### Install Cluster Operator using Helm

A Helm chart is available for the installation of the Strimzi Cluster Operator. 

1. Create the namespaces for the Cluster Operator and Kafka cluster

    ```bash
    kubectl create namespace strimzi-operator
    kubectl create namespace kafka
    ```

1. Create a values.yaml file to override specific configurations for the Helm chart

    ```bash
    cat <<EOF > values.yaml
    replicas: 3
    watchNamespaces: 
      - kafka
    leaderElection:
      enabled: true
    podDisruptionBudget:
      enabled: true
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: name
                  operator: In
                  values:
                    - strimzi-cluster-operator
            topologyKey: topology.kubernetes.io/zone
    EOF
    ```
     
1. Install the Strimzi Cluster Operator using [`helm install`][helm-install]

    ```bash
    helm install strimzi-cluster-operator oci://quay.io/strimzi-helm/strimzi-kafka-operator \
        --namespace strimzi-operator \
        --values values.yaml
    ```

1. Verify the Strimzi Cluster Operator is deployed and all pods are a running state

    ```bash
    kubectl get pods -n strimzi-operator
    ```

### Strimzi Drain Cleaner

Strimzi Drain Cleaner ensures smooth node draining by preventing Kafka partition replicas from becoming under-replicated, maintaining cluster health and reliability. Drainer Cleaner can also be deployed with multiple replicas and pod disruption budgets to ensure availability in case of a zonal outage or cluster upgrade

A Helm chart is available for the installation of Strimzi Drain Cleaner:

1. Create the namespaces for the Cluster Operator and Kafka cluster

    ```bash
    kubectl create namespace strimzi-drain-cleaner
    ```

1. Create a values.yaml file to override specific configurations for the Helm chart

    ```bash
    cat <<EOF > values.yaml
    replicaCount: 3
    namespace:
      create: false
    podDisruptionBudget:
      create: true
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
                - key: app
                  operator: In
                  values:
                    - strimzi-drain-cleaner
            topologyKey: topology.kubernetes.io/zone
    EOF
    ```
     
1. Install the Strimzi Drain Cleaner using [`helm install`][helm-install]

    ```bash
    helm install strimzi-drain-cleaner oci://quay.io/strimzi-helm/strimzi-drain-cleaner \
        --namespace strimzi-drain-cleaner \
        --values values.yaml
    ```

1. Verify the Strimzi Drain Cleaner is deployed and all pods are in a running state

    ```bash
    kubectl get pods -n strimzi-drain-cleaner
    ```

## Kafka Cluster Deployment


The Strimzi Cluster Operator allows us to declaratively define a Kafka Cluster on AKS using custom resource definitions. Beginning with Strimzi 0.46, Kafka clusters no longer require ZooKeeper. Instead, Kafka clusters will leverage KRaft. A KRaft Kafka Cluster consists of Kafka brokers, which handle data storage and processing, and Kafka controllers, which manage metadata using the Raft consensus protocol, eliminating the need for ZooKeeper. Strimzi uses a KafkaNodePool customer resource to create the brokers and controllers that will make up the Kafka cluster. KafkaNodePools must be assigned a specific role: broker, controller, or both. For the desired architecture, two KafkaNodePools are defined for broker and controllers, respectively, with a minimum of three (3) replicas. This setup simplifies the architecture, improves scalability, and enhances fault tolerance. Each broker and controller exists as its own respective set of pods within AKS and can be scaled or changed independently to meet workload demands.

<---INSERT Possible Diagram>

The KafkaNodePools are configured with Topology Spread Constraints so that the resulting pods are evenly distributed across availability zones and AKS nodes. This ensures resiliency in case of an availability zone outage or a node outage. A pod affinity rule with a *preferredDuringScheduling* rule is also configured between broker pods and controller pods. This ensures optimal use of node resources within a given availability zone if cluster autoscaler is enabled.  Otherwise, if a new node is added, there could be a scenario that a broker pod runs on an existing node and the controller pod on the scaled node. 

Persistent volume claims are established using the Storage Class generated by Azure Container Storage. Adjustments to the volume sizes should be made to fit your workload needs. The broker KafkaNodePool leverages two volumes, one for messages and the other for metadata.

### Deploy Kafka Node Pools

1. Create two (2) Kafka Node Pools, one for brokers and controllers, respectively

    ```bash
    kubectl apply -n kafka -f - <<EOF
    apiVersion: kafka.strimzi.io/v1beta2
    kind: KafkaNodePool
    metadata:
      name: controller
      labels:
        strimzi.io/cluster: kafka-aks-cluster
    spec:
      replicas: 3
      roles:
        - controller
      template:
        pod:
          metadata:
            labels:
              kafkaRole: controller
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - kafka
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchLabels:
                        kafkaRole: broker
                    topologyKey: kubernetes.io/hostname
          topologySpreadConstraints:
            - labelSelector:
                matchLabels:
                  kafkaRole: controller
              maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
            - labelSelector:
                matchLabels:
                  kafkaRole: controller
              maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
      storage:
        type: jbod
        volumes:
          - id: 0
            type: persistent-claim
            size: 2Gi  #adjust to fit workload needs
            kraftMetadata: shared
            deleteClaim: false
            class: acstor-azuredisk-zr
    ---
    apiVersion: kafka.strimzi.io/v1beta2
    kind: KafkaNodePool
    metadata:
      name: broker
      labels:
        strimzi.io/cluster: kafka-aks-cluster
    spec:
      replicas: 3
      roles:
        - broker
      template:
        pod:
          metadata:
            labels:
              kafkaRole: broker
          affinity:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - kafka
            podAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
                - weight: 100
                  podAffinityTerm:
                    labelSelector:
                      matchLabels:
                        kafkaRole: controller
                    topologyKey: kubernetes.io/hostname
          topologySpreadConstraints:
            - labelSelector:
                matchLabels:
                  kafkaRole: broker
              maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
            - labelSelector:
                matchLabels:
                  kafkaRole: broker
              maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
      storage:
        type: jbod
        volumes:
          - id: 0
            type: persistent-claim
            size: 2Gi #adjust to fit workload needs
            deleteClaim: false
            class: acstor-azuredisk-zr
          - id: 1
            type: persistent-claim
            size: 2Gi #adjust to fit workload needs
            kraftMetadata: shared
            deleteClaim: false
            class: acstor-azuredisk-zr
    EOF
    ```

Once the KafkaNodePools are defined, a Kafka Cluster object is created that is associated to the KafkaNodePools. The Kafka Cluster object is where Cruise Control, jmxPrometheusExporter, listeners, and Kafka specific configurations are declared. 

### Deploy Kafka Cluster

1. Before creating the Kafka cluster, create a ConfigMap for the Kafka cluster's jmxPrometheusExporter configuration to enable metrics to be sent to Azure Managed Prometheus

    ```bash
    kubectl apply -n kafka -f - <<EOF
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: kafka-metrics
      labels:
        app: strimzi
    data:
      kafka-metrics-config.yml: |
        # See https://github.com/prometheus/jmx_exporter for more info about JMX Prometheus Exporter metrics
        lowercaseOutputName: true
        rules:
        # Special cases and very specific rules
        - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value
          name: kafka_server_$1_$2
          type: GAUGE
          labels:
            clientId: "$3"
            topic: "$4"
            partition: "$5"
        - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><>Value
          name: kafka_server_$1_$2
          type: GAUGE
          labels:
            clientId: "$3"
            broker: "$4:$5"
        - pattern: kafka.server<type=(.+), cipher=(.+), protocol=(.+), listener=(.+), networkProcessor=(.+)><>connections
          name: kafka_server_$1_connections_tls_info
          type: GAUGE
          labels:
            cipher: "$2"
            protocol: "$3"
            listener: "$4"
            networkProcessor: "$5"
        - pattern: kafka.server<type=(.+), clientSoftwareName=(.+), clientSoftwareVersion=(.+), listener=(.+), networkProcessor=(.+)><>connections
          name: kafka_server_$1_connections_software
          type: GAUGE
          labels:
            clientSoftwareName: "$2"
            clientSoftwareVersion: "$3"
            listener: "$4"
            networkProcessor: "$5"
        - pattern: "kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+-total):"
          name: kafka_server_$1_$4
          type: COUNTER
          labels:
            listener: "$2"
            networkProcessor: "$3"
        - pattern: "kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+):"
          name: kafka_server_$1_$4
          type: GAUGE
          labels:
            listener: "$2"
            networkProcessor: "$3"
        - pattern: kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+-total)
          name: kafka_server_$1_$4
          type: COUNTER
          labels:
            listener: "$2"
            networkProcessor: "$3"
        - pattern: kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+)
          name: kafka_server_$1_$4
          type: GAUGE
          labels:
            listener: "$2"
            networkProcessor: "$3"
        # Some percent metrics use MeanRate attribute
        # Ex) kafka.server<type=(KafkaRequestHandlerPool), name=(RequestHandlerAvgIdlePercent)><>MeanRate
        - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>MeanRate
          name: kafka_$1_$2_$3_percent
          type: GAUGE
        # Generic gauges for percents
        - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>Value
          name: kafka_$1_$2_$3_percent
          type: GAUGE
        - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*, (.+)=(.+)><>Value
          name: kafka_$1_$2_$3_percent
          type: GAUGE
          labels:
            "$4": "$5"
        # Generic per-second counters with 0-2 key/value pairs
        - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)><>Count
          name: kafka_$1_$2_$3_total
          type: COUNTER
          labels:
            "$4": "$5"
            "$6": "$7"
        - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+)><>Count
          name: kafka_$1_$2_$3_total
          type: COUNTER
          labels:
            "$4": "$5"
        - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*><>Count
          name: kafka_$1_$2_$3_total
          type: COUNTER
        # Generic gauges with 0-2 key/value pairs
        - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Value
          name: kafka_$1_$2_$3
          type: GAUGE
          labels:
            "$4": "$5"
            "$6": "$7"
        - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Value
          name: kafka_$1_$2_$3
          type: GAUGE
          labels:
            "$4": "$5"
        - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Value
          name: kafka_$1_$2_$3
          type: GAUGE
        # Emulate Prometheus 'Summary' metrics for the exported 'Histogram's.
        # Note that these are missing the '_sum' metric!
        - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Count
          name: kafka_$1_$2_$3_count
          type: COUNTER
          labels:
            "$4": "$5"
            "$6": "$7"
        - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)><>(\d+)thPercentile
          name: kafka_$1_$2_$3
          type: GAUGE
          labels:
            "$4": "$5"
            "$6": "$7"
            quantile: "0.$8"
        - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Count
          name: kafka_$1_$2_$3_count
          type: COUNTER
          labels:
            "$4": "$5"
        - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*)><>(\d+)thPercentile
          name: kafka_$1_$2_$3
          type: GAUGE
          labels:
            "$4": "$5"
            quantile: "0.$6"
        - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Count
          name: kafka_$1_$2_$3_count
          type: COUNTER
        - pattern: kafka.(\w+)<type=(.+), name=(.+)><>(\d+)thPercentile
          name: kafka_$1_$2_$3
          type: GAUGE
          labels:
            quantile: "0.$4"
        # KRaft overall related metrics
        # distinguish between always increasing COUNTER (total and max) and variable GAUGE (all others) metrics
        - pattern: "kafka.server<type=raft-metrics><>(.+-total|.+-max):"
          name: kafka_server_raftmetrics_$1
          type: COUNTER
        - pattern: "kafka.server<type=raft-metrics><>(current-state): (.+)"
          name: kafka_server_raftmetrics_$1
          value: 1
          type: UNTYPED
          labels:
            $1: "$2"
        - pattern: "kafka.server<type=raft-metrics><>(.+):"
          name: kafka_server_raftmetrics_$1
          type: GAUGE
        # KRaft "low level" channels related metrics
        # distinguish between always increasing COUNTER (total and max) and variable GAUGE (all others) metrics
        - pattern: "kafka.server<type=raft-channel-metrics><>(.+-total|.+-max):"
          name: kafka_server_raftchannelmetrics_$1
          type: COUNTER
        - pattern: "kafka.server<type=raft-channel-metrics><>(.+):"
          name: kafka_server_raftchannelmetrics_$1
          type: GAUGE
        # Broker metrics related to fetching metadata topic records in KRaft mode
        - pattern: "kafka.server<type=broker-metadata-metrics><>(.+):"
          name: kafka_server_brokermetadatametrics_$1
          type: GAUGE
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: cruise-control-metrics
      labels:
        app: strimzi
    data:
      metrics-config.yml: |
        # See https://github.com/prometheus/jmx_exporter for more info about JMX Prometheus Exporter metrics
        lowercaseOutputName: true
        rules:
        - pattern: kafka.cruisecontrol<name=(.+)><>(\w+)
          name: kafka_cruisecontrol_$1_$2
          type: GAUGE
    EOF
    ```



### Config Map for Managed Prometheus 
### Pod Monitor for Managed Prometheus
### Kafka Node Pools, Kafka Cluster, Topic/User Operator, and Cruise Control 
### Kafka Rebalance
### Kafka Topic (declaratively create demo topic)

### Kafka Exporter

## Accessing the Kafka Cluster
### via ClusterIP
### Via ELB
### Via ILB + Private Endpoint
